{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pawel\\anaconda3\\envs\\env_torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 5216/5216 [00:00<00:00, 17045.74it/s]\n",
      "Resolving data files: 100%|██████████| 624/624 [00:00<00:00, 312320.49it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir=\"./datasets/chest_xray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 5216\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 624\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "labels = labels = dataset[\"train\"].features[\"label\"].names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "  label2id[i] = label\n",
    "  id2label[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NORMAL', 1: 'PNEUMONIA'}\n",
      "{'NORMAL': 0, 'PNEUMONIA': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "resizer = RandomResizedCrop(size)\n",
    "normalize = Normalize(image_processor.image_mean, image_processor.image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_transforms = Compose([resizer, ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "  examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "  del examples[\"image\"]\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 5216\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "  return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "  \"google/vit-base-patch16-224-in21k\",\n",
    "  num_labels=len(labels),\n",
    "  id2label=id2label,\n",
    "  label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir = \"pneumonia_model\",\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\",\n",
    "  learning_rate=5e-5,\n",
    "  per_device_train_batch_size=12,\n",
    "  per_device_eval_batch_size=12,\n",
    "  num_train_epochs=10,\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"accuracy\",\n",
    "  remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  data_collator=DefaultDataCollator(),\n",
    "  train_dataset=dataset[\"train\"],\n",
    "  eval_dataset=dataset[\"test\"],\n",
    "  tokenizer=image_processor,\n",
    "  compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 435/4350 [03:13<23:10,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37469497323036194, 'eval_accuracy': 0.8798076923076923, 'eval_runtime': 14.3549, 'eval_samples_per_second': 43.469, 'eval_steps_per_second': 3.622, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 500/4350 [03:40<26:16,  2.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.248, 'learning_rate': 4.4252873563218394e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 870/4350 [06:20<19:38,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31853964924812317, 'eval_accuracy': 0.9150641025641025, 'eval_runtime': 11.5778, 'eval_samples_per_second': 53.896, 'eval_steps_per_second': 4.491, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1000/4350 [07:15<21:51,  2.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1716, 'learning_rate': 3.850574712643678e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 30%|███       | 1305/4350 [09:32<16:52,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31958839297294617, 'eval_accuracy': 0.8990384615384616, 'eval_runtime': 11.9235, 'eval_samples_per_second': 52.334, 'eval_steps_per_second': 4.361, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1500/4350 [10:48<18:50,  2.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1357, 'learning_rate': 3.275862068965517e-05, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 1740/4350 [12:34<14:34,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25209927558898926, 'eval_accuracy': 0.9246794871794872, 'eval_runtime': 11.3581, 'eval_samples_per_second': 54.939, 'eval_steps_per_second': 4.578, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2000/4350 [14:16<14:42,  2.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.124, 'learning_rate': 2.7011494252873566e-05, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 2175/4350 [15:36<11:25,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32331418991088867, 'eval_accuracy': 0.9038461538461539, 'eval_runtime': 11.463, 'eval_samples_per_second': 54.436, 'eval_steps_per_second': 4.536, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 2500/4350 [17:44<12:04,  2.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1144, 'learning_rate': 2.1264367816091954e-05, 'epoch': 5.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 2610/4350 [18:38<08:54,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27962639927864075, 'eval_accuracy': 0.8974358974358975, 'eval_runtime': 11.4941, 'eval_samples_per_second': 54.289, 'eval_steps_per_second': 4.524, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3000/4350 [21:11<09:06,  2.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0987, 'learning_rate': 1.5517241379310346e-05, 'epoch': 6.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|███████   | 3045/4350 [21:39<06:39,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.214422345161438, 'eval_accuracy': 0.9342948717948718, 'eval_runtime': 11.2741, 'eval_samples_per_second': 55.348, 'eval_steps_per_second': 4.612, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 80%|████████  | 3480/4350 [24:41<04:37,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2941562533378601, 'eval_accuracy': 0.9118589743589743, 'eval_runtime': 11.402, 'eval_samples_per_second': 54.727, 'eval_steps_per_second': 4.561, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 3500/4350 [24:49<05:23,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0963, 'learning_rate': 9.770114942528738e-06, 'epoch': 8.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 3915/4350 [27:42<02:18,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34147024154663086, 'eval_accuracy': 0.907051282051282, 'eval_runtime': 11.3741, 'eval_samples_per_second': 54.861, 'eval_steps_per_second': 4.572, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 4000/4350 [28:17<02:14,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0817, 'learning_rate': 4.022988505747127e-06, 'epoch': 9.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "100%|██████████| 4350/4350 [45:06<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2847822308540344, 'eval_accuracy': 0.9118589743589743, 'eval_runtime': 13.1616, 'eval_samples_per_second': 47.411, 'eval_steps_per_second': 3.951, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4350/4350 [45:07<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2707.5956, 'train_samples_per_second': 19.264, 'train_steps_per_second': 1.607, 'train_loss': 0.1291817969837408, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4350, training_loss=0.1291817969837408, metrics={'train_runtime': 2707.5956, 'train_samples_per_second': 19.264, 'train_steps_per_second': 1.607, 'train_loss': 0.1291817969837408, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val = dataset['validation'][:]\n",
    "\n",
    "dataset_val['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = dataset_val[\"image\"]\n",
    "\n",
    "# dataset_val['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0386438 -0.986711 ] 0\n",
      "[-0.13800992  0.19367892] 1\n",
      "[ 0.20198247 -0.15564975] 0\n",
      "[-0.95488036  0.848078  ] 1\n",
      "[-1.2200586  1.0992448] 1\n",
      "[-0.5879604  0.5493035] 1\n",
      "[ 1.4296321 -1.0805426] 0\n",
      "[ 2.228238  -2.0731947] 0\n",
      "[-2.9751143  2.8376467] 1\n",
      "[-2.969958  2.867535] 1\n",
      "[-1.8585757  1.7708261] 1\n",
      "[-2.1129858  2.0069702] 1\n",
      "[-2.2598677  2.1570873] 1\n",
      "[-2.9136753  2.78434  ] 1\n",
      "[-2.8000977  2.695844 ] 1\n",
      "[-2.7378416  2.681501 ] 1\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "for i in range(0, 16):  \n",
    "  image = dataset_val[\"pixel_values\"][i]\n",
    "  \n",
    "  pred = model(image[None, ...])\n",
    "  \n",
    "  logits = pred.logits.detach().numpy()[0]\n",
    "  pred_class = np.argmax(logits)\n",
    "  \n",
    "  print(logits, pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
