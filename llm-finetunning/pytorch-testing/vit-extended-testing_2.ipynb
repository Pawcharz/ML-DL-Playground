{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Device & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pawel\\anaconda3\\envs\\env_torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 5216/5216 [00:00<00:00, 18982.56it/s]\n",
      "Resolving data files: 100%|██████████| 624/624 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"imagefolder\", data_dir=\"./datasets/chest_xray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 5216\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 624\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "labels = labels = raw_dataset[\"train\"].features[\"label\"].names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "  label2id[i] = label\n",
    "  id2label[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NORMAL', 1: 'PNEUMONIA'}\n",
      "{'NORMAL': 0, 'PNEUMONIA': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "resizer = RandomResizedCrop(size)\n",
    "normalize = Normalize(image_processor.image_mean, image_processor.image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_transforms = Compose([resizer, ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "  examples[\"image\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "  # del examples[\"image\"]\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 5216\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 624\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "  return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82770"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from own_model import CompositeModel\n",
    "from training_own import get_model_params\n",
    "from torch import nn\n",
    "\n",
    "own_layer = nn.Sequential(\n",
    "  nn.LayerNorm(1000),\n",
    "  nn.Linear(1000, 64),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(64, 128),\n",
    "  nn.Dropout(0.1),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(128, 64),\n",
    "  nn.Dropout(0.25),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(64, 2)\n",
    ")\n",
    "\n",
    "model = CompositeModel(own_layer)\n",
    "model.eval()\n",
    "# saved\n",
    "get_model_params(model.additional_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=16, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model.additional_layers.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "accuracy_metric = Accuracy(task='multiclass', num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_own import train_one_epoch, train_many_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0: true/predicted: 0/0\n",
      "index 1: true/predicted: 1/0\n",
      "index 2: true/predicted: 1/0\n",
      "index 3: true/predicted: 1/0\n",
      "index 4: true/predicted: 1/0\n",
      "index 5: true/predicted: 0/0\n",
      "index 6: true/predicted: 0/0\n",
      "index 7: true/predicted: 0/0\n",
      "index 8: true/predicted: 0/0\n",
      "index 9: true/predicted: 1/0\n",
      "index 10: true/predicted: 0/0\n",
      "index 11: true/predicted: 1/0\n",
      "index 12: true/predicted: 1/0\n",
      "index 13: true/predicted: 1/0\n",
      "index 14: true/predicted: 0/0\n",
      "index 15: true/predicted: 1/0\n",
      "index 16: true/predicted: 1/0\n",
      "index 17: true/predicted: 1/0\n",
      "index 18: true/predicted: 1/0\n",
      "index 19: true/predicted: 0/0\n",
      "index 20: true/predicted: 1/0\n",
      "index 21: true/predicted: 1/0\n",
      "index 22: true/predicted: 0/0\n",
      "index 23: true/predicted: 1/0\n",
      "index 24: true/predicted: 1/0\n",
      "testing accuracy: 0.36\n"
     ]
    }
   ],
   "source": [
    "from training_own import evaluate_model\n",
    "\n",
    "N_EXAMPLES = 25\n",
    "\n",
    "testing_fragment = dataset['test'].shuffle(seed=1)[:N_EXAMPLES]\n",
    "\n",
    "evaluate_model(model, testing_fragment=testing_fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 50 loss: 0.5416188043355942 training_accuracy: 0.7400000095367432\n",
      "  batch 100 loss: 0.5228772228956222 training_accuracy: 0.75\n",
      "  batch 150 loss: 0.5333236253261566 training_accuracy: 0.7450000047683716\n",
      "  batch 200 loss: 0.5165081006288529 training_accuracy: 0.75\n",
      "  batch 250 loss: 0.5390014296770096 training_accuracy: 0.7362499833106995\n",
      "  batch 300 loss: 0.529529277086258 training_accuracy: 0.7324999570846558\n",
      "LOSS train 0.529529277086258 valid 0.6311175227165222 ACCURACY validation 0.625\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 0.49056779861450195 training_accuracy: 0.7649999856948853\n",
      "  batch 100 loss: 0.5032666021585465 training_accuracy: 0.7437499761581421\n",
      "  batch 150 loss: 0.4890257292985916 training_accuracy: 0.7437499761581421\n",
      "  batch 200 loss: 0.51743632376194 training_accuracy: 0.7262499928474426\n",
      "  batch 250 loss: 0.49279205620288846 training_accuracy: 0.7475000023841858\n",
      "  batch 300 loss: 0.4918600368499756 training_accuracy: 0.7462499737739563\n",
      "LOSS train 0.4918600368499756 valid 0.6037630438804626 ACCURACY validation 0.625\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 0.4955085951089859 training_accuracy: 0.7362499833106995\n",
      "  batch 100 loss: 0.4784117287397385 training_accuracy: 0.7549999952316284\n",
      "  batch 150 loss: 0.4501295703649521 training_accuracy: 0.7737500071525574\n",
      "  batch 200 loss: 0.44517977684736254 training_accuracy: 0.7737500071525574\n",
      "  batch 250 loss: 0.4560344135761261 training_accuracy: 0.7574999928474426\n",
      "  batch 300 loss: 0.46613933473825453 training_accuracy: 0.7587499618530273\n",
      "LOSS train 0.46613933473825453 valid 0.583007276058197 ACCURACY validation 0.6506410241127014\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.4388356512784958 training_accuracy: 0.7912499904632568\n",
      "  batch 100 loss: 0.4353558337688446 training_accuracy: 0.7899999618530273\n",
      "  batch 150 loss: 0.4328593197464943 training_accuracy: 0.8062499761581421\n",
      "  batch 200 loss: 0.42325575530529025 training_accuracy: 0.7987499833106995\n",
      "  batch 250 loss: 0.45071996212005616 training_accuracy: 0.7862499952316284\n",
      "  batch 300 loss: 0.4053626790642738 training_accuracy: 0.824999988079071\n",
      "LOSS train 0.4053626790642738 valid 0.5672099590301514 ACCURACY validation 0.6875\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.4296846294403076 training_accuracy: 0.8199999928474426\n",
      "  batch 100 loss: 0.40613477528095243 training_accuracy: 0.8312499523162842\n",
      "  batch 150 loss: 0.4143128353357315 training_accuracy: 0.8075000047683716\n",
      "  batch 200 loss: 0.40819042444229126 training_accuracy: 0.8262499570846558\n",
      "  batch 250 loss: 0.3944553366303444 training_accuracy: 0.8262499570846558\n",
      "  batch 300 loss: 0.3810886138677597 training_accuracy: 0.8549999594688416\n",
      "LOSS train 0.3810886138677597 valid 0.5232701301574707 ACCURACY validation 0.7227564454078674\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.37276826947927477 training_accuracy: 0.8549999594688416\n",
      "  batch 100 loss: 0.38109278947114944 training_accuracy: 0.8412500023841858\n",
      "  batch 150 loss: 0.3687895074486732 training_accuracy: 0.8537499904632568\n",
      "  batch 200 loss: 0.3660953617095947 training_accuracy: 0.8574999570846558\n",
      "  batch 250 loss: 0.3786368060112 training_accuracy: 0.84375\n",
      "  batch 300 loss: 0.36115995079278945 training_accuracy: 0.8474999666213989\n",
      "LOSS train 0.36115995079278945 valid 0.5106096267700195 ACCURACY validation 0.7564102411270142\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.3928811722993851 training_accuracy: 0.8462499976158142\n",
      "  batch 100 loss: 0.3604948380589485 training_accuracy: 0.84375\n",
      "  batch 150 loss: 0.35695386677980423 training_accuracy: 0.8549999594688416\n",
      "  batch 200 loss: 0.34322229743003846 training_accuracy: 0.8637499809265137\n",
      "  batch 250 loss: 0.34067193627357484 training_accuracy: 0.8512499928474426\n",
      "  batch 300 loss: 0.3151056525111198 training_accuracy: 0.8812499642372131\n",
      "LOSS train 0.3151056525111198 valid 0.4760734438896179 ACCURACY validation 0.7788461446762085\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.31214329943060876 training_accuracy: 0.8787499666213989\n",
      "  batch 100 loss: 0.3425841614603996 training_accuracy: 0.856249988079071\n",
      "  batch 150 loss: 0.32875189170241353 training_accuracy: 0.8624999523162842\n",
      "  batch 200 loss: 0.3543723472952843 training_accuracy: 0.8487499952316284\n",
      "  batch 250 loss: 0.3090594652295113 training_accuracy: 0.8737499713897705\n",
      "  batch 300 loss: 0.31286710262298584 training_accuracy: 0.8787499666213989\n",
      "LOSS train 0.31286710262298584 valid 0.4832388758659363 ACCURACY validation 0.7516025900840759\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.3220622968673706 training_accuracy: 0.8624999523162842\n",
      "  batch 100 loss: 0.3342753165960312 training_accuracy: 0.856249988079071\n",
      "  batch 150 loss: 0.3166863361001015 training_accuracy: 0.8762499690055847\n",
      "  batch 200 loss: 0.2909072181582451 training_accuracy: 0.8812499642372131\n",
      "  batch 250 loss: 0.30604922443628313 training_accuracy: 0.8700000047683716\n",
      "  batch 300 loss: 0.28794842660427095 training_accuracy: 0.8862499594688416\n",
      "LOSS train 0.28794842660427095 valid 0.47675156593322754 ACCURACY validation 0.7676281929016113\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.3276155400276184 training_accuracy: 0.8687499761581421\n",
      "  batch 100 loss: 0.30569859698414803 training_accuracy: 0.8774999976158142\n",
      "  batch 150 loss: 0.30977003306150436 training_accuracy: 0.8675000071525574\n",
      "  batch 200 loss: 0.2944160407781601 training_accuracy: 0.8725000023841858\n",
      "  batch 250 loss: 0.2878710986673832 training_accuracy: 0.887499988079071\n",
      "  batch 300 loss: 0.25854634404182436 training_accuracy: 0.9074999690055847\n",
      "LOSS train 0.25854634404182436 valid 0.47398436069488525 ACCURACY validation 0.7724359035491943\n"
     ]
    }
   ],
   "source": [
    "train_many_epochs(\n",
    "  10,\n",
    "  model=model,\n",
    "  training_loader=training_loader,\n",
    "  validation_loader=validation_loader,\n",
    "  optimizer=optimizer,\n",
    "  loss_fn=loss_fn,\n",
    "  accuracy_metric=accuracy_metric,\n",
    "  cuda_device=device,\n",
    "  epoch_index=0,\n",
    "  logging_frequency=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_many_epochs(\n",
    "  2,\n",
    "  model=model,\n",
    "  training_loader=training_loader,\n",
    "  validation_loader=validation_loader,\n",
    "  optimizer=optimizer,\n",
    "  loss_fn=loss_fn,\n",
    "  accuracy_metric=accuracy_metric,\n",
    "  cuda_device=device,\n",
    "  epoch_index=0,\n",
    "  logging_frequency=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
