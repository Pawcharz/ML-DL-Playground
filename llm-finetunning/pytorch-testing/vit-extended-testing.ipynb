{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Device & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 5216/5216 [00:00<00:00, 23723.41it/s]\n",
      "Resolving data files: 100%|██████████| 624/624 [00:00<00:00, 312357.76it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"imagefolder\", data_dir=\"./datasets/chest_xray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 5216\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 624\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "labels = labels = raw_dataset[\"train\"].features[\"label\"].names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "  label2id[i] = label\n",
    "  id2label[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NORMAL', 1: 'PNEUMONIA'}\n",
      "{'NORMAL': 0, 'PNEUMONIA': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "resizer = RandomResizedCrop(size)\n",
    "normalize = Normalize(image_processor.image_mean, image_processor.image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_transforms = Compose([resizer, ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "  examples[\"image\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "  # del examples[\"image\"]\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 5216\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 624\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "  return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "class MyCompositeModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyCompositeModel, self).__init__()\n",
    "    \n",
    "    self.pretrained = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "    self.my_new_layers = nn.Sequential(\n",
    "      nn.Linear(1000, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 2)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.pretrained(x).logits\n",
    "    x = self.my_new_layers(x)\n",
    "    return x\n",
    "  \n",
    "model = MyCompositeModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=16, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.my_new_layers.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "accuracy_metric = Accuracy(task='multiclass', num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Epoch Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "  running_loss = 0.\n",
    "  running_accuracy = 0.\n",
    "  last_loss = 0.\n",
    "\n",
    "  # Here, we use enumerate(training_loader) instead of\n",
    "  # iter(training_loader) so that we can track the batch\n",
    "  # index and do some intra-epoch reporting\n",
    "  for i, data in enumerate(training_loader):\n",
    "  \n",
    "    # Every data instance is an input + label pair\n",
    "    inputs = data['image'].to(device)\n",
    "    labels = data['label'].to(device)\n",
    "  \n",
    "    # Zero your gradients for every batch!\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Make predictions for this batch\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Compute the loss and its gradients\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    training_accuracy = accuracy_metric(outputs, labels)\n",
    "\n",
    "    # Adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Gather data and report\n",
    "    running_loss += loss.item()\n",
    "    running_accuracy += training_accuracy\n",
    "    \n",
    "    # print('batch {}', i)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "      last_loss = running_loss / 10 # loss per batch\n",
    "      last_accuracy = running_accuracy / 10 # accuracy per batch\n",
    "      print('  batch {} loss: {} training_accuracy: {}'.format(i + 1, last_loss, last_accuracy))\n",
    "      tb_x = epoch_index * len(training_loader) + i + 1\n",
    "      tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "      running_loss = 0.\n",
    "      running_accuracy = 0.\n",
    "  \n",
    "  return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Epochs Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many_epochs(epochs, writer):\n",
    "  best_vloss = 1_000_000.\n",
    "\n",
    "  for epoch_number in range(epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vacc = 0.0\n",
    "    \n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "      for i, vdata in enumerate(validation_loader):\n",
    "        vinputs = vdata['image'].to(device)\n",
    "        vlabels = vdata['label'].to(device)\n",
    "        voutputs = model(vinputs)\n",
    "        \n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "        \n",
    "        vacc = accuracy_metric(voutputs, vlabels)\n",
    "        running_vacc += vacc\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vacc = running_vacc / (i + 1)\n",
    "    print('LOSS train {} valid {} ACCURACY validation {}'.format(avg_loss, avg_vloss, avg_vacc))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars(\n",
    "      'Training vs. Validation Loss',\n",
    "      { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "      epoch_number + 1\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "      best_vloss = avg_vloss\n",
    "      model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "      torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "sum_writer = SummaryWriter('runs/chest_trainer_{}'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 0.07893961071968078 training_accuracy: 0.01875000074505806\n",
      "  batch 11 loss: 0.6197352081537246 training_accuracy: 0.637499988079071\n",
      "  batch 21 loss: 0.5389519900083541 training_accuracy: 0.762499988079071\n",
      "  batch 31 loss: 0.5545953154563904 training_accuracy: 0.75\n",
      "  batch 41 loss: 0.5299925386905671 training_accuracy: 0.762499988079071\n",
      "  batch 51 loss: 0.5513634532690048 training_accuracy: 0.7250000238418579\n",
      "  batch 61 loss: 0.541490113735199 training_accuracy: 0.737500011920929\n",
      "  batch 71 loss: 0.5416361808776855 training_accuracy: 0.7125000357627869\n",
      "  batch 81 loss: 0.5312318652868271 training_accuracy: 0.7250000238418579\n",
      "  batch 91 loss: 0.4416340708732605 training_accuracy: 0.8062500357627869\n",
      "  batch 101 loss: 0.5152628064155579 training_accuracy: 0.71875\n",
      "  batch 111 loss: 0.5291282564401627 training_accuracy: 0.7250000238418579\n",
      "  batch 121 loss: 0.45343744456768037 training_accuracy: 0.8187500238418579\n",
      "  batch 131 loss: 0.5031083509325981 training_accuracy: 0.7437500357627869\n",
      "  batch 141 loss: 0.526752182841301 training_accuracy: 0.7562500238418579\n",
      "  batch 151 loss: 0.4672800570726395 training_accuracy: 0.7875000238418579\n",
      "  batch 161 loss: 0.5021343320608139 training_accuracy: 0.75\n",
      "  batch 171 loss: 0.48197165727615354 training_accuracy: 0.7437500357627869\n",
      "  batch 181 loss: 0.4479402363300323 training_accuracy: 0.7875000238418579\n",
      "  batch 191 loss: 0.4792231500148773 training_accuracy: 0.7562500238418579\n",
      "  batch 201 loss: 0.41789546310901643 training_accuracy: 0.793749988079071\n",
      "  batch 211 loss: 0.44987779259681704 training_accuracy: 0.762499988079071\n",
      "  batch 221 loss: 0.4285061687231064 training_accuracy: 0.793749988079071\n",
      "  batch 231 loss: 0.3851674944162369 training_accuracy: 0.8187500238418579\n",
      "  batch 241 loss: 0.3649042576551437 training_accuracy: 0.8500000238418579\n",
      "  batch 251 loss: 0.3763799875974655 training_accuracy: 0.8187500238418579\n",
      "  batch 261 loss: 0.4922312408685684 training_accuracy: 0.7875000238418579\n",
      "  batch 271 loss: 0.4475202590227127 training_accuracy: 0.824999988079071\n",
      "  batch 281 loss: 0.4514735370874405 training_accuracy: 0.793749988079071\n",
      "  batch 291 loss: 0.4013802409172058 training_accuracy: 0.856249988079071\n",
      "  batch 301 loss: 0.3858220636844635 training_accuracy: 0.84375\n",
      "  batch 311 loss: 0.3768274158239365 training_accuracy: 0.856249988079071\n",
      "  batch 321 loss: 0.41640199422836305 training_accuracy: 0.8187500238418579\n",
      "LOSS train 0.41640199422836305 valid 0.47573089599609375 ACCURACY validation 0.7836538553237915\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.027677643299102783 training_accuracy: 0.10000000149011612\n",
      "  batch 11 loss: 0.33033007234334943 training_accuracy: 0.8687500357627869\n",
      "  batch 21 loss: 0.3234516277909279 training_accuracy: 0.862500011920929\n",
      "  batch 31 loss: 0.3444374516606331 training_accuracy: 0.862500011920929\n",
      "  batch 41 loss: 0.37435885667800906 training_accuracy: 0.8375000357627869\n",
      "  batch 51 loss: 0.3317252978682518 training_accuracy: 0.875\n",
      "  batch 61 loss: 0.3182703420519829 training_accuracy: 0.893750011920929\n",
      "  batch 71 loss: 0.35087407678365706 training_accuracy: 0.875\n",
      "  batch 81 loss: 0.36908979192376135 training_accuracy: 0.8375000357627869\n",
      "  batch 91 loss: 0.35186125338077545 training_accuracy: 0.8375000357627869\n",
      "  batch 101 loss: 0.28288038074970245 training_accuracy: 0.8812500238418579\n",
      "  batch 111 loss: 0.35028261691331863 training_accuracy: 0.84375\n",
      "  batch 121 loss: 0.35464419722557067 training_accuracy: 0.9000000357627869\n",
      "  batch 131 loss: 0.40446199625730517 training_accuracy: 0.824999988079071\n",
      "  batch 141 loss: 0.23050522357225417 training_accuracy: 0.90625\n",
      "  batch 151 loss: 0.3081380322575569 training_accuracy: 0.887499988079071\n",
      "  batch 161 loss: 0.39851694703102114 training_accuracy: 0.8375000357627869\n",
      "  batch 171 loss: 0.3066965818405151 training_accuracy: 0.8500000238418579\n",
      "  batch 181 loss: 0.28554215282201767 training_accuracy: 0.90625\n",
      "  batch 191 loss: 0.2848364919424057 training_accuracy: 0.8812500238418579\n",
      "  batch 201 loss: 0.31406799554824827 training_accuracy: 0.8500000238418579\n",
      "  batch 211 loss: 0.2686127632856369 training_accuracy: 0.9000000357627869\n",
      "  batch 221 loss: 0.3117482215166092 training_accuracy: 0.8687500357627869\n",
      "  batch 231 loss: 0.29165367037057877 training_accuracy: 0.875\n",
      "  batch 241 loss: 0.29773467034101486 training_accuracy: 0.862500011920929\n",
      "  batch 251 loss: 0.3326322630047798 training_accuracy: 0.8500000238418579\n",
      "  batch 261 loss: 0.26198066025972366 training_accuracy: 0.9312500357627869\n",
      "  batch 271 loss: 0.3477397710084915 training_accuracy: 0.831250011920929\n",
      "  batch 281 loss: 0.30667528212070466 training_accuracy: 0.8812500238418579\n",
      "  batch 291 loss: 0.2900078445672989 training_accuracy: 0.893750011920929\n",
      "  batch 301 loss: 0.3065178729593754 training_accuracy: 0.862500011920929\n",
      "  batch 311 loss: 0.23732746690511702 training_accuracy: 0.893750011920929\n",
      "  batch 321 loss: 0.27803020030260084 training_accuracy: 0.9000000357627869\n",
      "LOSS train 0.27803020030260084 valid 0.44234219193458557 ACCURACY validation 0.7900640964508057\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.012303534150123595 training_accuracy: 0.10000000149011612\n",
      "  batch 11 loss: 0.3157831564545631 training_accuracy: 0.8812500238418579\n",
      "  batch 21 loss: 0.23292159289121628 training_accuracy: 0.893750011920929\n",
      "  batch 31 loss: 0.33595509082078934 training_accuracy: 0.84375\n",
      "  batch 41 loss: 0.2525195375084877 training_accuracy: 0.918749988079071\n",
      "  batch 51 loss: 0.2591666728258133 training_accuracy: 0.887499988079071\n",
      "  batch 61 loss: 0.24319349676370622 training_accuracy: 0.918749988079071\n",
      "  batch 71 loss: 0.2877663657069206 training_accuracy: 0.862500011920929\n",
      "  batch 81 loss: 0.2516753673553467 training_accuracy: 0.9125000238418579\n",
      "  batch 91 loss: 0.21899462714791298 training_accuracy: 0.893750011920929\n",
      "  batch 101 loss: 0.29774987101554873 training_accuracy: 0.8812500238418579\n",
      "  batch 111 loss: 0.2703747652471066 training_accuracy: 0.875\n",
      "  batch 121 loss: 0.28560794293880465 training_accuracy: 0.887499988079071\n",
      "  batch 131 loss: 0.29030665904283526 training_accuracy: 0.8812500238418579\n",
      "  batch 141 loss: 0.17505977675318718 training_accuracy: 0.9312500357627869\n",
      "  batch 151 loss: 0.286446875333786 training_accuracy: 0.8687500357627869\n",
      "  batch 161 loss: 0.2532091155648232 training_accuracy: 0.8812500238418579\n",
      "  batch 171 loss: 0.25322223678231237 training_accuracy: 0.9000000357627869\n",
      "  batch 181 loss: 0.30632380247116087 training_accuracy: 0.875\n",
      "  batch 191 loss: 0.34529646933078767 training_accuracy: 0.8375000357627869\n",
      "  batch 201 loss: 0.2858238875865936 training_accuracy: 0.856249988079071\n",
      "  batch 211 loss: 0.2971334770321846 training_accuracy: 0.8375000357627869\n",
      "  batch 221 loss: 0.19656684771180152 training_accuracy: 0.918749988079071\n",
      "  batch 231 loss: 0.22015826031565666 training_accuracy: 0.90625\n",
      "  batch 241 loss: 0.3090374022722244 training_accuracy: 0.8812500238418579\n",
      "  batch 251 loss: 0.2554098181426525 training_accuracy: 0.8812500238418579\n",
      "  batch 261 loss: 0.2851815171539783 training_accuracy: 0.893750011920929\n",
      "  batch 271 loss: 0.25462791323661804 training_accuracy: 0.887499988079071\n",
      "  batch 281 loss: 0.2209869172424078 training_accuracy: 0.90625\n",
      "  batch 291 loss: 0.22879577279090882 training_accuracy: 0.90625\n",
      "  batch 301 loss: 0.28422008007764815 training_accuracy: 0.875\n",
      "  batch 311 loss: 0.2817010723054409 training_accuracy: 0.875\n",
      "  batch 321 loss: 0.20034438520669937 training_accuracy: 0.9312500357627869\n",
      "LOSS train 0.20034438520669937 valid 0.37934380769729614 ACCURACY validation 0.817307710647583\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 0.010650601983070374 training_accuracy: 0.10000000149011612\n",
      "  batch 11 loss: 0.2642983190715313 training_accuracy: 0.8687500357627869\n",
      "  batch 21 loss: 0.28098855912685394 training_accuracy: 0.893750011920929\n",
      "  batch 31 loss: 0.20179768577218055 training_accuracy: 0.9312500357627869\n",
      "  batch 41 loss: 0.28820573091506957 training_accuracy: 0.90625\n",
      "  batch 51 loss: 0.2527610778808594 training_accuracy: 0.8812500238418579\n",
      "  batch 61 loss: 0.26077576726675034 training_accuracy: 0.875\n",
      "  batch 71 loss: 0.22579725608229637 training_accuracy: 0.925000011920929\n",
      "  batch 81 loss: 0.2225768633186817 training_accuracy: 0.893750011920929\n",
      "  batch 91 loss: 0.19171756580471994 training_accuracy: 0.9312500357627869\n",
      "  batch 101 loss: 0.1698567494750023 training_accuracy: 0.9437500238418579\n",
      "  batch 111 loss: 0.2892941579222679 training_accuracy: 0.875\n",
      "  batch 121 loss: 0.2553337037563324 training_accuracy: 0.90625\n",
      "  batch 131 loss: 0.2545442350208759 training_accuracy: 0.887499988079071\n",
      "  batch 141 loss: 0.2580824658274651 training_accuracy: 0.856249988079071\n",
      "  batch 151 loss: 0.293563149869442 training_accuracy: 0.84375\n",
      "  batch 161 loss: 0.14915206655859947 training_accuracy: 0.9375\n",
      "  batch 171 loss: 0.298288069665432 training_accuracy: 0.893750011920929\n",
      "  batch 181 loss: 0.1902180477976799 training_accuracy: 0.918749988079071\n",
      "  batch 191 loss: 0.25071738958358764 training_accuracy: 0.8812500238418579\n",
      "  batch 201 loss: 0.24255412667989731 training_accuracy: 0.9000000357627869\n",
      "  batch 211 loss: 0.2905596747994423 training_accuracy: 0.875\n",
      "  batch 221 loss: 0.24164937287569047 training_accuracy: 0.9000000357627869\n",
      "  batch 231 loss: 0.22160159572958946 training_accuracy: 0.9000000357627869\n",
      "  batch 241 loss: 0.26295324191451075 training_accuracy: 0.8687500357627869\n",
      "  batch 251 loss: 0.25542347729206083 training_accuracy: 0.918749988079071\n",
      "  batch 261 loss: 0.23969840928912162 training_accuracy: 0.918749988079071\n",
      "  batch 271 loss: 0.24724386930465697 training_accuracy: 0.887499988079071\n",
      "  batch 281 loss: 0.2424198493361473 training_accuracy: 0.9000000357627869\n",
      "  batch 291 loss: 0.21299598067998887 training_accuracy: 0.9125000238418579\n",
      "  batch 301 loss: 0.23566884547472 training_accuracy: 0.887499988079071\n",
      "  batch 311 loss: 0.2626346543431282 training_accuracy: 0.887499988079071\n",
      "  batch 321 loss: 0.2099714159965515 training_accuracy: 0.918749988079071\n",
      "LOSS train 0.2099714159965515 valid 0.5618629455566406 ACCURACY validation 0.7564102411270142\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.008976801484823226 training_accuracy: 0.10000000149011612\n",
      "  batch 11 loss: 0.28409790247678757 training_accuracy: 0.8812500238418579\n",
      "  batch 21 loss: 0.2549340434372425 training_accuracy: 0.875\n",
      "  batch 31 loss: 0.22553835734724997 training_accuracy: 0.875\n",
      "  batch 41 loss: 0.19381990171968938 training_accuracy: 0.9125000238418579\n",
      "  batch 51 loss: 0.23412389382719995 training_accuracy: 0.893750011920929\n",
      "  batch 61 loss: 0.2910299703478813 training_accuracy: 0.8500000238418579\n",
      "  batch 71 loss: 0.2735048934817314 training_accuracy: 0.8812500238418579\n",
      "  batch 81 loss: 0.22090339735150338 training_accuracy: 0.9125000238418579\n",
      "  batch 91 loss: 0.21606841385364534 training_accuracy: 0.918749988079071\n",
      "  batch 101 loss: 0.21776960995048283 training_accuracy: 0.9125000238418579\n",
      "  batch 111 loss: 0.25729370266199114 training_accuracy: 0.9125000238418579\n",
      "  batch 121 loss: 0.2345670759677887 training_accuracy: 0.9125000238418579\n",
      "  batch 131 loss: 0.2524318628013134 training_accuracy: 0.90625\n",
      "  batch 141 loss: 0.2193027652800083 training_accuracy: 0.90625\n",
      "  batch 151 loss: 0.249417307972908 training_accuracy: 0.9125000238418579\n",
      "  batch 161 loss: 0.22700215131044388 training_accuracy: 0.9125000238418579\n",
      "  batch 171 loss: 0.32452327385544777 training_accuracy: 0.8687500357627869\n",
      "  batch 181 loss: 0.25369189828634264 training_accuracy: 0.887499988079071\n",
      "  batch 191 loss: 0.26685783416032793 training_accuracy: 0.887499988079071\n",
      "  batch 201 loss: 0.25906737372279165 training_accuracy: 0.887499988079071\n",
      "  batch 211 loss: 0.22634101063013076 training_accuracy: 0.918749988079071\n",
      "  batch 221 loss: 0.22959503047168256 training_accuracy: 0.9125000238418579\n",
      "  batch 231 loss: 0.22045412063598632 training_accuracy: 0.9312500357627869\n",
      "  batch 241 loss: 0.26085561960935594 training_accuracy: 0.893750011920929\n",
      "  batch 251 loss: 0.2532724946737289 training_accuracy: 0.918749988079071\n",
      "  batch 261 loss: 0.27000993862748146 training_accuracy: 0.90625\n",
      "  batch 271 loss: 0.2261863797903061 training_accuracy: 0.887499988079071\n",
      "  batch 281 loss: 0.26751207262277604 training_accuracy: 0.8687500357627869\n",
      "  batch 291 loss: 0.23872036039829253 training_accuracy: 0.887499988079071\n",
      "  batch 301 loss: 0.2091173894703388 training_accuracy: 0.925000011920929\n",
      "  batch 311 loss: 0.23740940392017365 training_accuracy: 0.893750011920929\n",
      "  batch 321 loss: 0.21930264513939618 training_accuracy: 0.9000000357627869\n",
      "LOSS train 0.21930264513939618 valid 0.47168493270874023 ACCURACY validation 0.7948718070983887\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.011461380124092101 training_accuracy: 0.08749999850988388\n",
      "  batch 11 loss: 0.1941646821796894 training_accuracy: 0.925000011920929\n",
      "  batch 21 loss: 0.15823511518537997 training_accuracy: 0.949999988079071\n",
      "  batch 31 loss: 0.26278864331543444 training_accuracy: 0.8812500238418579\n",
      "  batch 41 loss: 0.25093876086175443 training_accuracy: 0.887499988079071\n",
      "  batch 51 loss: 0.20404869951307775 training_accuracy: 0.9125000238418579\n",
      "  batch 61 loss: 0.16337256208062173 training_accuracy: 0.9437500238418579\n",
      "  batch 71 loss: 0.27882098220288754 training_accuracy: 0.893750011920929\n",
      "  batch 81 loss: 0.29635693281888964 training_accuracy: 0.8812500238418579\n",
      "  batch 91 loss: 0.3142159316688776 training_accuracy: 0.875\n",
      "  batch 101 loss: 0.21850028783082961 training_accuracy: 0.90625\n",
      "  batch 111 loss: 0.23182096928358079 training_accuracy: 0.9000000357627869\n",
      "  batch 121 loss: 0.22847125083208084 training_accuracy: 0.9125000238418579\n",
      "  batch 131 loss: 0.20778377801179887 training_accuracy: 0.9312500357627869\n",
      "  batch 141 loss: 0.2159471085295081 training_accuracy: 0.90625\n",
      "  batch 151 loss: 0.22620202898979186 training_accuracy: 0.9000000357627869\n",
      "  batch 161 loss: 0.30590005666017533 training_accuracy: 0.875\n",
      "  batch 171 loss: 0.23530795499682428 training_accuracy: 0.90625\n",
      "  batch 181 loss: 0.27524668872356417 training_accuracy: 0.856249988079071\n",
      "  batch 191 loss: 0.27194047123193743 training_accuracy: 0.8687500357627869\n",
      "  batch 201 loss: 0.17617252469062805 training_accuracy: 0.9437500238418579\n",
      "  batch 211 loss: 0.24786697328090668 training_accuracy: 0.887499988079071\n",
      "  batch 221 loss: 0.244824381172657 training_accuracy: 0.875\n",
      "  batch 231 loss: 0.23127114474773408 training_accuracy: 0.918749988079071\n",
      "  batch 241 loss: 0.23249671943485736 training_accuracy: 0.887499988079071\n",
      "  batch 251 loss: 0.23224616870284082 training_accuracy: 0.90625\n",
      "  batch 261 loss: 0.20261543728411197 training_accuracy: 0.925000011920929\n",
      "  batch 271 loss: 0.23201075196266174 training_accuracy: 0.9125000238418579\n",
      "  batch 281 loss: 0.18152211904525756 training_accuracy: 0.949999988079071\n",
      "  batch 291 loss: 0.16939897760748862 training_accuracy: 0.9437500238418579\n",
      "  batch 301 loss: 0.20261540822684765 training_accuracy: 0.925000011920929\n",
      "  batch 311 loss: 0.21508867889642716 training_accuracy: 0.9000000357627869\n",
      "  batch 321 loss: 0.25181794911623 training_accuracy: 0.8687500357627869\n",
      "LOSS train 0.25181794911623 valid 0.7165535092353821 ACCURACY validation 0.7339743971824646\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.03343955874443054 training_accuracy: 0.08125000447034836\n",
      "  batch 11 loss: 0.21220777481794356 training_accuracy: 0.9000000357627869\n",
      "  batch 21 loss: 0.22350063174962997 training_accuracy: 0.918749988079071\n",
      "  batch 31 loss: 0.2777568601071835 training_accuracy: 0.887499988079071\n",
      "  batch 41 loss: 0.27799185998737813 training_accuracy: 0.893750011920929\n",
      "  batch 51 loss: 0.19855924211442472 training_accuracy: 0.9312500357627869\n",
      "  batch 61 loss: 0.19759017899632453 training_accuracy: 0.9125000238418579\n",
      "  batch 71 loss: 0.22346263080835344 training_accuracy: 0.893750011920929\n",
      "  batch 81 loss: 0.17460591122508048 training_accuracy: 0.949999988079071\n",
      "  batch 91 loss: 0.2319193109869957 training_accuracy: 0.9000000357627869\n",
      "  batch 101 loss: 0.1644293911755085 training_accuracy: 0.925000011920929\n",
      "  batch 111 loss: 0.16541380174458026 training_accuracy: 0.9312500357627869\n",
      "  batch 121 loss: 0.2775473713874817 training_accuracy: 0.90625\n",
      "  batch 131 loss: 0.20397747680544853 training_accuracy: 0.918749988079071\n",
      "  batch 141 loss: 0.21283138394355774 training_accuracy: 0.9000000357627869\n",
      "  batch 151 loss: 0.21410418748855592 training_accuracy: 0.9125000238418579\n",
      "  batch 161 loss: 0.17478274181485176 training_accuracy: 0.9312500357627869\n",
      "  batch 171 loss: 0.18953543826937674 training_accuracy: 0.9125000238418579\n",
      "  batch 181 loss: 0.2411135785281658 training_accuracy: 0.887499988079071\n",
      "  batch 191 loss: 0.21191544085741043 training_accuracy: 0.9125000238418579\n",
      "  batch 201 loss: 0.2801408238708973 training_accuracy: 0.8812500238418579\n",
      "  batch 211 loss: 0.22060152217745782 training_accuracy: 0.918749988079071\n",
      "  batch 221 loss: 0.19592056497931482 training_accuracy: 0.918749988079071\n",
      "  batch 231 loss: 0.2560804195702076 training_accuracy: 0.887499988079071\n",
      "  batch 241 loss: 0.2191647082567215 training_accuracy: 0.9125000238418579\n",
      "  batch 251 loss: 0.24925427585840226 training_accuracy: 0.9000000357627869\n",
      "  batch 261 loss: 0.14906291887164116 training_accuracy: 0.949999988079071\n",
      "  batch 271 loss: 0.16461956463754177 training_accuracy: 0.925000011920929\n",
      "  batch 281 loss: 0.2357735201716423 training_accuracy: 0.9125000238418579\n",
      "  batch 291 loss: 0.2000785417854786 training_accuracy: 0.9437500238418579\n",
      "  batch 301 loss: 0.22903212159872055 training_accuracy: 0.90625\n",
      "  batch 311 loss: 0.3065150879323483 training_accuracy: 0.8812500238418579\n",
      "  batch 321 loss: 0.18902116343379022 training_accuracy: 0.9375\n",
      "LOSS train 0.18902116343379022 valid 0.3330182731151581 ACCURACY validation 0.8477564454078674\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.00940084457397461 training_accuracy: 0.09375\n",
      "  batch 11 loss: 0.1786310389637947 training_accuracy: 0.9437500238418579\n",
      "  batch 21 loss: 0.22516770139336587 training_accuracy: 0.9000000357627869\n",
      "  batch 31 loss: 0.2646547257900238 training_accuracy: 0.887499988079071\n",
      "  batch 41 loss: 0.18268414512276648 training_accuracy: 0.949999988079071\n",
      "  batch 51 loss: 0.2250079445540905 training_accuracy: 0.925000011920929\n",
      "  batch 61 loss: 0.29339882731437683 training_accuracy: 0.887499988079071\n",
      "  batch 71 loss: 0.20033472031354904 training_accuracy: 0.9125000238418579\n",
      "  batch 81 loss: 0.2444751664996147 training_accuracy: 0.875\n",
      "  batch 91 loss: 0.19566687643527986 training_accuracy: 0.9375\n",
      "  batch 101 loss: 0.2011607840657234 training_accuracy: 0.925000011920929\n",
      "  batch 111 loss: 0.23106524348258972 training_accuracy: 0.893750011920929\n",
      "  batch 121 loss: 0.22195742130279542 training_accuracy: 0.887499988079071\n",
      "  batch 131 loss: 0.2554576739668846 training_accuracy: 0.893750011920929\n",
      "  batch 141 loss: 0.2158166728913784 training_accuracy: 0.918749988079071\n",
      "  batch 151 loss: 0.17206670641899108 training_accuracy: 0.949999988079071\n",
      "  batch 161 loss: 0.28981476426124575 training_accuracy: 0.893750011920929\n",
      "  batch 171 loss: 0.20097786113619803 training_accuracy: 0.9437500238418579\n",
      "  batch 181 loss: 0.23122526481747627 training_accuracy: 0.9125000238418579\n",
      "  batch 191 loss: 0.19938035756349565 training_accuracy: 0.925000011920929\n",
      "  batch 201 loss: 0.23205954916775226 training_accuracy: 0.918749988079071\n",
      "  batch 211 loss: 0.27236257269978525 training_accuracy: 0.90625\n",
      "  batch 221 loss: 0.16625629514455795 training_accuracy: 0.918749988079071\n",
      "  batch 231 loss: 0.21498369351029395 training_accuracy: 0.9312500357627869\n",
      "  batch 241 loss: 0.24075375497341156 training_accuracy: 0.9125000238418579\n",
      "  batch 251 loss: 0.26297747939825056 training_accuracy: 0.887499988079071\n",
      "  batch 261 loss: 0.24989657998085021 training_accuracy: 0.9125000238418579\n",
      "  batch 271 loss: 0.2533638119697571 training_accuracy: 0.887499988079071\n",
      "  batch 281 loss: 0.22826987244188784 training_accuracy: 0.9125000238418579\n",
      "  batch 291 loss: 0.2566851884126663 training_accuracy: 0.887499988079071\n",
      "  batch 301 loss: 0.17395884320139884 training_accuracy: 0.9312500357627869\n",
      "  batch 311 loss: 0.22568956837058068 training_accuracy: 0.893750011920929\n",
      "  batch 321 loss: 0.24568177685141562 training_accuracy: 0.918749988079071\n",
      "LOSS train 0.24568177685141562 valid 0.5145614147186279 ACCURACY validation 0.7932692170143127\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.006279357522726059 training_accuracy: 0.10000000149011612\n",
      "  batch 11 loss: 0.21535609364509584 training_accuracy: 0.893750011920929\n",
      "  batch 21 loss: 0.19091340377926827 training_accuracy: 0.9375\n",
      "  batch 31 loss: 0.2343920897692442 training_accuracy: 0.9125000238418579\n",
      "  batch 41 loss: 0.24502988159656525 training_accuracy: 0.9125000238418579\n",
      "  batch 51 loss: 0.22393962666392325 training_accuracy: 0.875\n",
      "  batch 61 loss: 0.28647720981389285 training_accuracy: 0.8687500357627869\n",
      "  batch 71 loss: 0.2088692769408226 training_accuracy: 0.925000011920929\n",
      "  batch 81 loss: 0.1553449511528015 training_accuracy: 0.9312500357627869\n",
      "  batch 91 loss: 0.17125989012420179 training_accuracy: 0.9125000238418579\n",
      "  batch 101 loss: 0.20904240012168884 training_accuracy: 0.925000011920929\n",
      "  batch 111 loss: 0.23154260069131852 training_accuracy: 0.893750011920929\n",
      "  batch 121 loss: 0.19376954808831215 training_accuracy: 0.9125000238418579\n",
      "  batch 131 loss: 0.19247715957462788 training_accuracy: 0.90625\n",
      "  batch 141 loss: 0.313389390707016 training_accuracy: 0.862500011920929\n",
      "  batch 151 loss: 0.2559732165187597 training_accuracy: 0.887499988079071\n",
      "  batch 161 loss: 0.2463573195040226 training_accuracy: 0.9125000238418579\n",
      "  batch 171 loss: 0.23433352634310722 training_accuracy: 0.9125000238418579\n",
      "  batch 181 loss: 0.18170871622860432 training_accuracy: 0.9437500238418579\n",
      "  batch 191 loss: 0.15339072458446026 training_accuracy: 0.956250011920929\n",
      "  batch 201 loss: 0.16132568418979645 training_accuracy: 0.918749988079071\n",
      "  batch 211 loss: 0.3183649003505707 training_accuracy: 0.8812500238418579\n",
      "  batch 221 loss: 0.2248113639652729 training_accuracy: 0.918749988079071\n",
      "  batch 231 loss: 0.1953325316309929 training_accuracy: 0.925000011920929\n",
      "  batch 241 loss: 0.15583832263946534 training_accuracy: 0.9437500238418579\n",
      "  batch 251 loss: 0.27172602638602256 training_accuracy: 0.9125000238418579\n",
      "  batch 261 loss: 0.2527722865343094 training_accuracy: 0.9000000357627869\n",
      "  batch 271 loss: 0.24815645739436148 training_accuracy: 0.9000000357627869\n",
      "  batch 281 loss: 0.19962093085050583 training_accuracy: 0.9125000238418579\n",
      "  batch 291 loss: 0.2548313304781914 training_accuracy: 0.9000000357627869\n",
      "  batch 301 loss: 0.23780104778707029 training_accuracy: 0.9312500357627869\n",
      "  batch 311 loss: 0.2375451758503914 training_accuracy: 0.90625\n",
      "  batch 321 loss: 0.1809882327914238 training_accuracy: 0.90625\n",
      "LOSS train 0.1809882327914238 valid 0.4001697301864624 ACCURACY validation 0.8381410241127014\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.009503173828125 training_accuracy: 0.10000000149011612\n",
      "  batch 11 loss: 0.21942260339856148 training_accuracy: 0.9000000357627869\n",
      "  batch 21 loss: 0.2596731223165989 training_accuracy: 0.893750011920929\n",
      "  batch 31 loss: 0.26667579263448715 training_accuracy: 0.875\n",
      "  batch 41 loss: 0.2121823847293854 training_accuracy: 0.893750011920929\n",
      "  batch 51 loss: 0.27095001973211763 training_accuracy: 0.9000000357627869\n",
      "  batch 61 loss: 0.17463322952389718 training_accuracy: 0.9125000238418579\n",
      "  batch 71 loss: 0.21070847436785697 training_accuracy: 0.918749988079071\n",
      "  batch 81 loss: 0.19933417849242688 training_accuracy: 0.9312500357627869\n",
      "  batch 91 loss: 0.22519882023334503 training_accuracy: 0.893750011920929\n",
      "  batch 101 loss: 0.21032355725765228 training_accuracy: 0.9312500357627869\n",
      "  batch 111 loss: 0.18759646788239479 training_accuracy: 0.925000011920929\n",
      "  batch 121 loss: 0.18446271121501923 training_accuracy: 0.925000011920929\n",
      "  batch 131 loss: 0.1439497157931328 training_accuracy: 0.949999988079071\n",
      "  batch 141 loss: 0.17739888653159142 training_accuracy: 0.9375\n",
      "  batch 151 loss: 0.262965153157711 training_accuracy: 0.9000000357627869\n",
      "  batch 161 loss: 0.2532137591391802 training_accuracy: 0.90625\n",
      "  batch 171 loss: 0.17062040641903878 training_accuracy: 0.925000011920929\n",
      "  batch 181 loss: 0.21585618257522582 training_accuracy: 0.9125000238418579\n",
      "  batch 191 loss: 0.16235562190413474 training_accuracy: 0.90625\n",
      "  batch 201 loss: 0.2548963725566864 training_accuracy: 0.893750011920929\n",
      "  batch 211 loss: 0.2317273274064064 training_accuracy: 0.9000000357627869\n",
      "  batch 221 loss: 0.24675979763269423 training_accuracy: 0.9125000238418579\n",
      "  batch 231 loss: 0.2595873884856701 training_accuracy: 0.856249988079071\n",
      "  batch 241 loss: 0.19102344661951065 training_accuracy: 0.925000011920929\n",
      "  batch 251 loss: 0.17372393868863584 training_accuracy: 0.925000011920929\n",
      "  batch 261 loss: 0.16426425129175187 training_accuracy: 0.9375\n",
      "  batch 271 loss: 0.34002598747611046 training_accuracy: 0.856249988079071\n",
      "  batch 281 loss: 0.183600003272295 training_accuracy: 0.918749988079071\n",
      "  batch 291 loss: 0.22168797999620438 training_accuracy: 0.90625\n",
      "  batch 301 loss: 0.3346163615584373 training_accuracy: 0.8687500357627869\n",
      "  batch 311 loss: 0.22807808592915535 training_accuracy: 0.9312500357627869\n",
      "  batch 321 loss: 0.1893614877015352 training_accuracy: 0.956250011920929\n",
      "LOSS train 0.1893614877015352 valid 0.426920086145401 ACCURACY validation 0.8381410241127014\n"
     ]
    }
   ],
   "source": [
    "train_many_epochs(10, sum_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -  1 0\n",
      "1  -  1 0\n",
      "2  -  1 0\n",
      "3  -  1 0\n",
      "4  -  1 0\n",
      "5  -  1 0\n",
      "6  -  0 0\n",
      "7  -  0 0\n",
      "8  -  1 1\n",
      "9  -  1 1\n",
      "10  -  1 1\n",
      "11  -  1 1\n",
      "12  -  1 1\n",
      "13  -  1 1\n",
      "14  -  1 1\n",
      "15  -  1 1\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cpu')\n",
    "\n",
    "for i, image in enumerate(raw_dataset['validation']):\n",
    "  input = dataset['validation'][i]['image']\n",
    "  label_true = dataset['validation'][i]['label']\n",
    "  logits = model(input[None, ...]).detach().numpy()\n",
    "  label_pred = np.argmax(logits)\n",
    "  print(i, \" - \" ,label_pred, label_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
